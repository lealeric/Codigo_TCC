{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METODOLOGIA BASEADA EM CIÊNCIA DE REDES PARA ANÁLISE DE COMUNIDADES EM REDES SOCIAIS COM GEOLOCALIZAÇÃO\n",
    "\n",
    "## Autor: ERIC LEAL\n",
    "\n",
    "### Detalhes do Projeto\n",
    "Em desenvolvimento...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "from pandas import json_normalize\n",
    "import networkx as nx\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import gravis as gv\n",
    "# import pyproj\n",
    "import utm\n",
    "import math\n",
    "# import io\n",
    "# import gc\n",
    "# from scipy.spatial import ConvexHull, Voronoi, voronoi_plot_2d\n",
    "# from scipy import stats\n",
    "\n",
    "# from functools import partial\n",
    "from IPython.display import display, clear_output\n",
    "# import concurrent.futures\n",
    "from tqdm.contrib.concurrent import thread_map\n",
    "\n",
    "import tweepy\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Required Libraries\n",
    "from tqdm import tqdm as tqdmBasic\n",
    "# import time\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns; sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurando a API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the keys from the .env file\n",
    "consumer_key = os.environ.get('TWITTER_API_KEY')\n",
    "consumer_secret = os.environ.get('TWITTER_API_KEY_SECRET')\n",
    "access_token = os.environ.get('TWITTER_ACCESS_TOKEN')\n",
    "access_token_secret = os.environ.get('TWITTER_ACCESS_TOKEN_SECRET')\n",
    "bearer_token = os.environ.get('TWITTER_BEARER_TOKEN')\n",
    "\n",
    "# Authenticate with the twitter API\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# Create the API object\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando dados existententes caso existam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionario a ser preenchido com chaves sendo o author_id e os valores sendo o username\n",
    "if os.path.exists(r'E://Dict_Author_Id_to_Username.json'):\n",
    "    with open(r'E://Dict_Author_Id_to_Username.json', 'r') as fp:\n",
    "        author_id_to_username = json.load(fp)\n",
    "        print(f\"Foram carregadas {len(author_id_to_username)} chaves do arquivo JSON\")\n",
    "else:\n",
    "    author_id_to_username = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean = pd.read_csv(r\"E:/DataFrameUsernameMeansMedians.csv\", sep=\";\", header=None)\n",
    "df_mean.columns = [\"username\", \"mean_lat\", \"mean_lon\", \"median_lat\", \"median_lon\", \"mean_X\", \"mean_Y\", \"median_X\", \"median_Y\"]\n",
    "df_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTopAuthorsProjected = pd.read_csv(\"E://TopAuthorsProjected.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_edgelist(\"D:\\\\Documentos\\\\data_and_code\\\\all_data_lisbon\\\\graphWithCoords.txt\", create_using=nx.DiGraph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando métodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def community_layout(g, partition):\n",
    "    \"\"\"\n",
    "    Compute the layout for a modular graph.\n",
    "\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "    g -- networkx.Graph or networkx.DiGraph instance\n",
    "        graph to plot\n",
    "\n",
    "    partition -- dict mapping int node -> int community\n",
    "        graph partitions\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pos -- dict mapping int node -> (float x, float y)\n",
    "        node positions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    pos_communities = _position_communities(g, partition, scale=3.)\n",
    "\n",
    "    pos_nodes = _position_nodes(g, partition, scale=1.)\n",
    "\n",
    "    # combine positions\n",
    "    pos = dict()\n",
    "    for node in g.nodes():\n",
    "        pos[node] = pos_communities[node] + pos_nodes[node]\n",
    "\n",
    "    return pos\n",
    "\n",
    "def _position_communities(g, partition, **kwargs):\n",
    "\n",
    "    # create a weighted graph, in which each node corresponds to a community,\n",
    "    # and each edge weight to the number of edges between communities\n",
    "    between_community_edges = _find_between_community_edges(g, partition)\n",
    "\n",
    "    communities = set(partition.values())\n",
    "    hypergraph = nx.DiGraph()\n",
    "    hypergraph.add_nodes_from(communities)\n",
    "    for (ci, cj), edges in between_community_edges.items():\n",
    "        hypergraph.add_edge(ci, cj, weight=len(edges))\n",
    "\n",
    "    # find layout for communities\n",
    "    pos_communities = nx.spring_layout(hypergraph, **kwargs)\n",
    "\n",
    "    # set node positions to position of community\n",
    "    pos = dict()\n",
    "    for node, community in partition.items():\n",
    "        pos[node] = pos_communities[community]\n",
    "\n",
    "    return pos\n",
    "\n",
    "def _find_between_community_edges(g, partition):\n",
    "\n",
    "    edges = dict()\n",
    "\n",
    "    for (ni, nj) in g.edges():\n",
    "        ci = partition[ni]\n",
    "        cj = partition[nj]\n",
    "\n",
    "        if ci != cj:\n",
    "            try:\n",
    "                edges[(ci, cj)] += [(ni, nj)]\n",
    "            except KeyError:\n",
    "                edges[(ci, cj)] = [(ni, nj)]\n",
    "\n",
    "    return edges\n",
    "\n",
    "def _position_nodes(g, partition, **kwargs):\n",
    "    \"\"\"\n",
    "    Positions nodes within communities.\n",
    "    \"\"\"\n",
    "\n",
    "    communities = dict()\n",
    "    for node, community in partition.items():\n",
    "        try:\n",
    "            communities[community] += [node]\n",
    "        except KeyError:\n",
    "            communities[community] = [node]\n",
    "\n",
    "    pos = dict()\n",
    "    for ci, nodes in communities.items():\n",
    "        subgraph = g.subgraph(nodes)\n",
    "        pos_subgraph = nx.spring_layout(subgraph, **kwargs)\n",
    "        pos.update(pos_subgraph)\n",
    "\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(input_path) -> list:\n",
    "    \"\"\"\n",
    "    Read list of objects from a JSON lines file.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.rstrip('\\n|\\r')))\n",
    "    print('Loaded {} records from {}'.format(len(data), input_path))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geraDictMentionsByUser(data_frame: pd.DataFrame, mentionsUnicas = True) -> dict:\n",
    "    dataEditado = data_frame['entities_mentions']\n",
    "    dataEditado.dropna(inplace=True)\n",
    "    # dataEditado.reset_index(drop=True, inplace=True)\n",
    "    dataEditado = dataEditado.to_frame()\n",
    "    \n",
    "    mentionsUsuario = {}\n",
    "\n",
    "    # Percorrendo teste1 e adicionando os usernames no dicionário usando o id do usuário como chave\n",
    "        \n",
    "    for mention in dataEditado.itertuples():\n",
    "        usuariosMencionados = []\n",
    "        \n",
    "        coluna = 'author_id' #'username' if data_frame['username'].at[mention.Index] != '' else 'author_id'\n",
    "        \n",
    "        if data_frame[coluna].at[mention.Index] not in mentionsUsuario:\n",
    "            mentionsUsuario[data_frame[coluna].at[mention.Index]] = []\n",
    "            \n",
    "        for usuario in mention[1]:\n",
    "            usuariosMencionados.append(usuario['username'])\n",
    "            mentionsUsuario[data_frame[coluna].at[mention.Index]].extend(usuariosMencionados)\n",
    "        \n",
    "    if mentionsUnicas:\n",
    "        mentionsUsuario = {k: list(dict.fromkeys(v)) for k, v in mentionsUsuario.items()}\n",
    "        \n",
    "    return mentionsUsuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geraDictLinkTweets(data_frame) -> list:\n",
    "    dataEditado = data_frame['entities_urls']\n",
    "    dataEditado.dropna(inplace=True)\n",
    "    # dataEditado.reset_index(drop=True, inplace=True)\n",
    "    dataEditado = dataEditado.to_frame()\n",
    "    \n",
    "    linksTweets = {}\n",
    "\n",
    "    # Percorrendo teste1 e adicionando os usernames no dicionário usando o id do usuário como chave\n",
    "        \n",
    "    contador = 1\n",
    "    for link in dataEditado.itertuples():\n",
    "        print(f\"Link {contador} de {dataEditado.shape[0]}\")\n",
    "        \n",
    "        linksTweet = []\n",
    "        if data_frame['author_id'].at[link.Index] not in linksTweets:\n",
    "            linksTweets[data_frame['author_id'].at[link.Index]] = []\n",
    "            \n",
    "        for linkTweet in link[1]:\n",
    "            linksTweet.append(linkTweet['expanded_url'])\n",
    "            linksTweets[data_frame['author_id'].at[link.Index]].extend(linksTweet)\n",
    "        contador += 1\n",
    "        \n",
    "    return linksTweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnMediaDictListas(dict : dict) -> float:\n",
    "    soma = 0\n",
    "    for key in dict:\n",
    "        soma += len(dict[key])\n",
    "    return soma/len(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_username(author_id: str):\n",
    "    \"\"\"Recupera o username do usuário a partir do author_id e atualiza o dicionário author_id_to_username\n",
    "\n",
    "    Args:\n",
    "        author_id: O id do usuário \n",
    "    \"\"\"\n",
    "    if author_id in author_id_to_username.keys():\n",
    "        return\n",
    "    \n",
    "    author_id_to_username[author_id] = ''\n",
    "    try:\n",
    "        user = api.get_user(user_id=author_id)  \n",
    "        author_id_to_username[author_id] = user.screen_name\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnDataFrameWithUsernames(dataFrame : pd.DataFrame) -> pd.DataFrame:      \n",
    "    dataFrame['username'] = ''\n",
    "    \n",
    "    thread_map(get_username, [id for id in dataFrame['author_id']], max_workers=16, total=dataFrame.shape[0])\n",
    "        \n",
    "    for index, row in dataFrame.iterrows():\n",
    "        dataFrame['username'].at[index] = author_id_to_username[row['author_id']]\n",
    "    \n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_histogram_directed(G, in_degree=False, out_degree=False):\n",
    "    \"\"\"Return a list of the frequency of each degree value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : Networkx graph\n",
    "       A graph\n",
    "    in_degree : bool\n",
    "    out_degree : bool\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    hist : list\n",
    "       A list of frequencies of degrees.\n",
    "       The degree values are the index in the list.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Note: the bins are width one, hence len(list) can be large\n",
    "    (Order(number_of_edges))\n",
    "    \"\"\"\n",
    "    nodes = G.nodes()\n",
    "    if in_degree:\n",
    "        in_degree = dict(G.in_degree())\n",
    "        degseq=[in_degree.get(k,0) for k in nodes]\n",
    "    elif out_degree:\n",
    "        out_degree = dict(G.out_degree())\n",
    "        degseq=[out_degree.get(k,0) for k in nodes]\n",
    "    else:\n",
    "        degseq=[v for k, v in G.degree()]\n",
    "    dmax=max(degseq)+1\n",
    "    freq= [ 0 for d in range(dmax) ]\n",
    "    for d in degseq:\n",
    "        freq[d] += 1\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _export_dictionary(dict : dict, path : str = 'E://', file_name : str = 'dict.json'):\n",
    "    \"\"\"Exporta um dicionário para um arquivo JSON\n",
    "\n",
    "    Args:\n",
    "        dict (dict): dicionário a ser exportado\n",
    "        path (str, optional): Caminho do arquivo de saída. Defaults to ''.\n",
    "        file_name (str, optional): Nome do arquivo de saída. Defaults to 'dict.json'.\n",
    "    \"\"\"                \n",
    "    with open(f\"{path}{file_name}\", 'w') as f:\n",
    "        json.dump(dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processRawData(data : dict | list[dict]) -> pd.DataFrame:\n",
    "    data_frame = json_normalize(data,record_path = ['data'], sep = '_', errors ='ignore')\n",
    "        \n",
    "    data_frame['lat'] = data_frame['geo_coordinates_coordinates'].str.get(1)\n",
    "    data_frame['lon'] = data_frame['geo_coordinates_coordinates'].str.get(0)\n",
    "\n",
    "    #cleaning fields that will not be used\n",
    "    try:\n",
    "        # del data_frame['entities_mentions']\n",
    "        del data_frame['context_annotations']\n",
    "        # del data_frame['entities_urls']\n",
    "        # del data_frame['entities_cashtags']\n",
    "        del data_frame['withheld_copyright']\n",
    "        del data_frame['withheld_country_codes']\n",
    "        del data_frame['withheld_scope']\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file: str) -> pd.DataFrame:\n",
    "    \"\"\"Processa um arquivo JSONL e retorna um dataframe\n",
    "\n",
    "    Args:\n",
    "        file (str): Caminho do arquivo JSONL\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe com os dados do arquivo JSONL\n",
    "    \"\"\"\n",
    "    print(f\"Processando arquivo {file}\")\n",
    "    df = json_normalize(load_jsonl(file), sep='_', errors='ignore')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_utm(df: pd.DataFrame, columnLat: str = \"latitude\", columnLon: str = \"longitude\", outColumnX: str = \"x\", outColumnY: str = \"y\") -> pd.DataFrame:\n",
    "  \"\"\"Converte as coordenadas de latitude e longitude para UTM zona 29.\n",
    "\n",
    "  Args:\n",
    "      df (pd.DataFrame): Dataframe com as coordenadas de latitude e longitude.\n",
    "      columnLat (str, optional): Coluna que contém a latitude. Defaults to \"latitude\".\n",
    "      columnLon (str, optional): Coluna que contém a longitude. Defaults to \"longitude\".\n",
    "      outColumnX (str, optional): Coluna saída em UTM para o eixo X. Defaults to \"x\".\n",
    "      outColumnY (str, optional): Coluna saída em UTM para o eixo Y. Defaults to \"y\".\n",
    "\n",
    "  Returns:\n",
    "      pd.DataFrame: Dataframe atualizado com as coordendas em UTM.\n",
    "  \"\"\"\n",
    "  # Obtém as coordenadas de latitude e longitude.\n",
    "\n",
    "  lat = df[columnLat].values\n",
    "  lon = df[columnLon].values\n",
    "\n",
    "  # Converte para UTM.\n",
    "\n",
    "  easting, northing, zone_number, zone_letter = utm.from_latlon(lat, lon)\n",
    "\n",
    "  # Adiciona as colunas `mean_X` e `mean_Y` ao dataframe.\n",
    "\n",
    "  df[outColumnX] = easting\n",
    "  df[outColumnY] = northing\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def associateMentions(index, row, dictMentions):\n",
    "    mentions = []\n",
    "    author_id = row['author_id']\n",
    "    username = row['username']\n",
    "    \n",
    "    if author_id in dictMentions:\n",
    "        mentions = dictMentions[author_id]\n",
    "    elif username in dictMentions:\n",
    "        mentions = dictMentions[username]\n",
    "    \n",
    "    return mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expanded_urls(row):\n",
    "    \"\"\"\n",
    "    Retorna uma lista com as URLs expandidas do tweet.\n",
    "\n",
    "    Args:\n",
    "        row: Uma linha do dataframe.\n",
    "\n",
    "    Returns:\n",
    "        Uma lista com as URLs expandidas do tweet.\n",
    "    \"\"\"\n",
    "\n",
    "    if row['entities_urls']:\n",
    "        expanded_urls = [url['expanded_url'] for url in row['entities_urls']]\n",
    "    else:\n",
    "        expanded_urls = []\n",
    "    return expanded_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def associateNodes(row):\n",
    "    G = nx.DiGraph()\n",
    "    vertex = row['username'] if row['username'] != '' else row['author_id']\n",
    "    \n",
    "    if vertex not in G.nodes():\n",
    "        G.add_node(vertex)\n",
    "        \n",
    "    if row['entities_mentions']:\n",
    "        for mention in row['entities_mentions']:\n",
    "            mention_username = mention['username']\n",
    "            if mention_username not in G.nodes():\n",
    "                G.add_node(mention_username)\n",
    "            G.add_edge(vertex, mention_username)\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateDistance(xP1: float, yP1: float, xP2: float, yP2: float):\n",
    "    P1 = [xP1, yP1]\n",
    "    P2 = [xP2, yP2]\n",
    "    \n",
    "    return math.dist(P1, P2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateDistances(graph, generalizationLimit = None):\n",
    "  nodes_with_median = [node for node in graph.nodes if \"median_X\" in graph.nodes[node]]\n",
    "  \n",
    "  print(f\"Existem {len(nodes_with_median)} nós com mediana.\")\n",
    "  \n",
    "  distances = {}\n",
    "  \n",
    "  for node in tqdmBasic(nodes_with_median):\n",
    "    for other_node in nodes_with_median:\n",
    "        if node == other_node:\n",
    "            continue\n",
    "        \n",
    "        # print(f\"Calculando distância entre {node} e {other_node}\")\n",
    "        \n",
    "        distance = calculateDistance(\n",
    "            graph.nodes[node][\"median_X\"], graph.nodes[node][\"median_Y\"], graph.nodes[other_node][\"median_X\"], graph.nodes[other_node][\"median_Y\"]\n",
    "        )\n",
    "        if generalizationLimit is not None:\n",
    "            generaralized_distance = int(distance//generalizationLimit)\n",
    "            if generaralized_distance not in distances.keys():\n",
    "                distances[generaralized_distance] = 1\n",
    "            else:\n",
    "                distances[generaralized_distance] += 1\n",
    "        else:\n",
    "            if distance not in distances.keys():\n",
    "                distances[distance] = 1\n",
    "            else:\n",
    "                distances[distance] += 1\n",
    "            \n",
    "        # distancesFromNode[node].append(distance)\n",
    "  \n",
    "  \n",
    "  return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotScatterEmpyricalComplementarDistribution(distances, output_path: str = 'E://', output_file_name: str = 'empyrical_complementar_distribution.png', show: bool = False, log: bool = False):\n",
    "    \"\"\"Plota o gráfico de dispersão da distribuição empírica complementar.\n",
    "\n",
    "    Args:\n",
    "        distances (list): Lista com as distâncias.\n",
    "        output_path (str): Caminho de saída do arquivo.\n",
    "        output_file_name (str): Nome do arquivo de saída.\n",
    "        show (bool): Se o gráfico deve ser exibido.\n",
    "    \"\"\"    \n",
    "    # sort the distances\n",
    "\n",
    "    # calculate the probability of each distance\n",
    "    if isinstance(distances, dict):\n",
    "        keys_in_order = sorted(distances.keys())\n",
    "        length_distances = sum(distances.values())\n",
    "        dist = 1\n",
    "        prob = []\n",
    "        print(f\"keys_in_order: {keys_in_order}\")\n",
    "        print(f\"length_distances: {length_distances}\")\n",
    "        \n",
    "        distances_list = [key for key, value in distances.items() for _ in tqdmBasic(range(value))]\n",
    "        n = len(distances_list)\n",
    "        prob = [1 - (i+1)/n for i in tqdmBasic(range(0,n))]\n",
    "                    \n",
    "        distances = list(distances.keys())\n",
    "        \n",
    "    else:\n",
    "        distances.sort()\n",
    "        if log == True:\n",
    "            prob = [np.log10(1 - (i/len(distances))) for i in tqdmBasic(range(len(distances)), desc=\"Calculando probabilidade\")]\n",
    "        else:\n",
    "            prob = [1 - (i/len(distances)) for i in tqdmBasic(range(len(distances)))]\n",
    "            \n",
    "    print(type(prob))\n",
    "\n",
    "    # plot the scatter graph\n",
    "    plt.figure(figsize=(180, 120))\n",
    "    plt.scatter(distances, prob)\n",
    "    plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.xlabel(\"Distância\")\n",
    "    plt.ylabel(\"Probabilidade\")\n",
    "    plt.savefig(f\"{output_path}{output_file_name}\", dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando o grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrameComUsernames = json_normalize(load_jsonl(r'D:\\\\Documentos\\\\data_and_code\\\\all_data_lisbon\\\\data_processed_data_1.jsonl.jsonl'), sep = '_', errors ='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictMentions = geraDictMentionsByUser(dataFrameComUsernames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "i = 1\n",
    "for index, row in dataFrameComUsernames.iterrows():\n",
    "    clear_output(wait=True)\n",
    "    display('Adicionando vértice '+str(i)+' of '+str(dataFrameComUsernames.shape[0]))\n",
    "    vertex = row['username'] if row['username'] != '' else row['author_id']\n",
    "    \n",
    "    if vertex not in G.nodes():\n",
    "        G.add_node(vertex)\n",
    "            \n",
    "    if row['author_id'] not in dictMentions.keys():\n",
    "        continue\n",
    "    \n",
    "    for mention in dictMentions[row['author_id']]:\n",
    "        if mention not in G.nodes():\n",
    "            G.add_node(mention)\n",
    "        G.add_edge(vertex, mention)\n",
    "    \n",
    "    i = i + 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
